{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc55ffc-c56b-4816-8a50-b0c31e534a3f",
   "metadata": {},
   "source": [
    "# parallel sweeps\n",
    "#### 9 Dec 2025\n",
    "creates dfs inf_all and null_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc307b5f-2393-4197-8de6-603bc608d68e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c581749-d64e-47c0-a329-56215f1363a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'with_plots.ipynb', 'vcf_stage']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ba17e-51f9-4002-82db-a163e6d33494",
   "metadata": {},
   "source": [
    "### get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b463b-6773-41cf-86ff-cf88557b82bd",
   "metadata": {},
   "source": [
    "#### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987e1ea6-c154-496d-bd48-c15dccbc9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798ae0aa-7e34-4f63-ae22-097a5709538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644b8672-21e4-4d71-82ba-80c4333ebb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecd7b6bd0d4497e9c714b024d2b0ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.05) # Simulate some work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63ecf0b-1d69-452c-a0e3-06268b8ab695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsinfer\n",
    "import tskit\n",
    "import msprime\n",
    "import tsdate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "from Bio import SeqIO, AlignIO\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "import subprocess, gzip, tempfile\n",
    "import shutil\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f122102-1025-43bc-a472-aa48954ad52f",
   "metadata": {},
   "source": [
    "#### define insert_proxy_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae0d152-381b-448e-b8fd-0382ae4c6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tsinfer\")\n",
    "\n",
    "def insert_proxy_samples(\n",
    "        self,\n",
    "        variant_data,\n",
    "        *,\n",
    "        sample_ids=None,\n",
    "        epsilon=None,\n",
    "        keep_ancestor_times=None,\n",
    "        allow_mutation=None,  # deprecated alias\n",
    "        **kwargs,\n",
    "):\n",
    "        \"\"\"\n",
    "        Take a set of samples from a :class:`.VariantData` instance and create additional\n",
    "        \"proxy sample ancestors\" from them, returning a new :class:`.AncestorData`\n",
    "        instance including both the current ancestors and the additional ancestors\n",
    "        at the appropriate time points.\n",
    "\n",
    "        A *proxy sample ancestor* is an ancestor based upon a known sample. At\n",
    "        sites used in the full inference process, the haplotype of this ancestor\n",
    "        is identical to that of the sample on which it is based. The time of the\n",
    "        ancestor is taken to be a fraction ``epsilon`` older than the sample on\n",
    "        which it is based.\n",
    "\n",
    "        A common use of this function is to provide ancestral nodes for anchoring\n",
    "        historical samples at the correct time when matching them into a tree\n",
    "        sequence during the :func:`tsinfer.match_samples` stage of inference.\n",
    "        For this reason, by default, the samples chosen from ``sample_data``\n",
    "        are those associated with historical (i.e. non-contemporary)\n",
    "        :ref:`individuals <sec_inference_data_model_individual>`. This can be\n",
    "        altered by using the ``sample_ids`` parameter.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The proxy sample ancestors inserted here will correspond to extra nodes\n",
    "            in the inferred tree sequence. At sites which are not used in the full\n",
    "            inference process (e.g. sites unique to a single historical sample),\n",
    "            these proxy sample ancestor nodes may have a different genotype from\n",
    "            their corresponding sample.\n",
    "\n",
    "        :param VariantData variant_data: The `VariantData` instance\n",
    "            from which to select the samples used to create extra ancestors.\n",
    "        :param list(int) sample_ids: A list of sample ids in the ``variant_data``\n",
    "            instance that will be selected to create the extra ancestors. If\n",
    "            ``None`` (default) select all the historical samples, i.e. those\n",
    "            associated with an :ref:`sec_inference_data_model_individual` whose\n",
    "            time is greater than zero. The order of ids is ignored, as are\n",
    "            duplicate ids.\n",
    "        :param list(float) epsilon: An list of small time increments\n",
    "            determining how much older each proxy sample ancestor is than the\n",
    "            corresponding sample listed in ``sample_ids``. A single value is also\n",
    "            allowed, in which case it is used as the time increment for all selected\n",
    "            proxy sample ancestors. If None (default) find :math:`{\\\\delta}t`, the\n",
    "            smallest time difference between the sample times and the next\n",
    "            oldest ancestor in the current :class:`.AncestorData` instance, setting\n",
    "            ``epsilon`` = :math:`{\\\\delta}t / 100` (or, if all selected samples\n",
    "            are at least as old as the oldest ancestor, take :math:`{\\\\delta}t`\n",
    "            to be the smallest non-zero time difference between existing ancestors).\n",
    "        :param bool keep_ancestor_times: If ``False`` (the default), the existing\n",
    "            times of the ancestors in the current :class:`.AncestorData` instance\n",
    "            may be increased so that derived states in the inserted proxy samples.\n",
    "            can have an ancestor with a mutation to that site above them (i.e. the\n",
    "            infinite sites assumption is maintained). This is useful when sites\n",
    "            times have been approximated by using their frequency. Alternatively,\n",
    "            if ``keep_ancestor_times`` is ``True``, existing ancestor times are\n",
    "            preserved, and inserted proxy sample ancestors are allowed to\n",
    "            possess derived alleles at sites where there are no pre-existing\n",
    "            mutations in older ancestors. This can lead to a de-novo mutation at a\n",
    "            site that also has a mutation elsewhere (i.e. breaking the infinite sites\n",
    "            assumption).\n",
    "        :param bool allow_mutation: Deprecated alias for `keep_ancestor_times`.\n",
    "        :param \\\\**kwargs: Further arguments passed to the constructor when creating\n",
    "            the new :class:`AncestorData` instance which will be returned.\n",
    "\n",
    "        :return: A new :class:`.AncestorData` object.\n",
    "        :rtype: AncestorData\n",
    "        \"\"\"\n",
    "        if allow_mutation is not None:\n",
    "            if keep_ancestor_times is not None:\n",
    "                raise ValueError(\n",
    "                    \"Cannot specify both `allow_mutation` and `keep_ancestor_times`\"\n",
    "                )\n",
    "            keep_ancestor_times = allow_mutation\n",
    "        self._check_finalised()\n",
    "        variant_data._check_finalised()\n",
    "        if self.sequence_length != variant_data.sequence_length:\n",
    "            raise ValueError(\"variant_data does not have the correct sequence length\")\n",
    "        used_sites = np.isin(variant_data.sites_position[:], self.sites_position[:])\n",
    "        if np.sum(used_sites) != self.num_sites:\n",
    "            raise ValueError(\"Genome positions in ancestors missing from variant_data\")\n",
    "\n",
    "        if sample_ids is None:\n",
    "            sample_ids = []\n",
    "            for i in variant_data.individuals():\n",
    "                if i.time > 0:\n",
    "                    sample_ids += i.samples\n",
    "        # sort by ID and make unique for quick haplotype access\n",
    "        sample_ids, unique_indices = np.unique(np.array(sample_ids), return_index=True)\n",
    "\n",
    "        sample_times = np.zeros(len(sample_ids), dtype=self.ancestors_time.dtype)\n",
    "        for i, s in enumerate(sample_ids):\n",
    "            sample = variant_data.sample(s)\n",
    "            if sample.individual != tskit.NULL:\n",
    "                sample_times[i] = variant_data.individual(sample.individual).time\n",
    "\n",
    "        if epsilon is not None:\n",
    "            epsilons = np.atleast_1d(epsilon)\n",
    "            if len(epsilons) == 1:\n",
    "                # all get the same epsilon\n",
    "                epsilons = np.repeat(epsilons, len(sample_ids))\n",
    "            else:\n",
    "                if len(epsilons) != len(unique_indices):\n",
    "                    raise ValueError(\n",
    "                        \"The number of epsilon values must equal the number of \"\n",
    "                        f\"sample_ids ({len(sample_ids)})\"\n",
    "                    )\n",
    "                epsilons = epsilons[unique_indices]\n",
    "\n",
    "        else:\n",
    "            anc_times = self.ancestors_time[:][::-1]  # find ascending time order\n",
    "            older_index = np.searchsorted(anc_times, sample_times, side=\"right\")\n",
    "            # Don't include times older than the oldest ancestor\n",
    "            allowed = older_index < self.num_ancestors\n",
    "            if np.sum(allowed) > 0:\n",
    "                delta_t = anc_times[older_index[allowed]] - sample_times[allowed]\n",
    "            else:\n",
    "                # All samples have times equal to or older than the oldest curr ancestor\n",
    "                time_diffs = np.diff(anc_times)\n",
    "                delta_t = np.min(time_diffs[time_diffs > 0])\n",
    "            epsilons = np.repeat(np.min(delta_t) / 100.0, len(sample_ids))\n",
    "\n",
    "        proxy_times = sample_times + epsilons\n",
    "        time_sorted_indexes = np.argsort(proxy_times)\n",
    "        reverse_time_sorted_indexes = time_sorted_indexes[::-1]\n",
    "        # In cases where we have more than a handful of samples to use as proxies, it is\n",
    "        # inefficient to access the haplotypes out of order, so we iterate and cache\n",
    "        # (caution: the haplotypes list may be quite large in this case)\n",
    "        haplotypes = [\n",
    "            h[1] for h in variant_data.haplotypes(\n",
    "                samples=sample_ids, sites=used_sites, recode_ancestral=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        new_anc_times = self.ancestors_time[:]  # this is a copy\n",
    "        if not keep_ancestor_times:\n",
    "            assert np.all(np.diff(self.ancestors_time) <= 0)\n",
    "            # Find the youngest (max) ancestor ID constrained by each sample haplotype\n",
    "            site_ancestor = -np.ones(self.num_sites, dtype=int)\n",
    "            anc_min_time = np.zeros(self.num_ancestors, dtype=self.ancestors_time.dtype)\n",
    "            # If (unusually) there are multiple ancestors for the same focal site, we\n",
    "            # can take the youngest\n",
    "            for ancestor_id, focal_sites in enumerate(self.ancestors_focal_sites):\n",
    "                site_ancestor[focal_sites] = ancestor_id\n",
    "            for hap_id in time_sorted_indexes:\n",
    "                derived_sites = haplotypes[hap_id] > 0\n",
    "                if np.sum(derived_sites) == 0:\n",
    "                    root = 0  # no derived sites, so only needs to be below the root\n",
    "                    for i, focal_sites in enumerate(self.ancestors_focal_sites):\n",
    "                        if len(focal_sites) > 0:\n",
    "                            if i > 0:\n",
    "                                root = i - 1\n",
    "                            anc_min_time[root] = proxy_times[hap_id] + epsilons[hap_id]\n",
    "                            break\n",
    "                else:\n",
    "                    max_anc_id = np.max(site_ancestor[derived_sites])  # youngest ancstr\n",
    "                    if max_anc_id >= 0:\n",
    "                        anc_min_time[max_anc_id] = proxy_times[hap_id] + epsilons[hap_id]\n",
    "            # Go from youngest to oldest, pushing up the times of the ancestors to\n",
    "            # achieve compatibility with infinite sites.\n",
    "            # TODO - replace with something mre efficient that uses time_diffs\n",
    "            for anc_id in range(self.num_ancestors - 1, -1, -1):\n",
    "                current_time = new_anc_times[anc_id]\n",
    "                if anc_min_time[anc_id] > current_time:\n",
    "                    new_anc_times[:(anc_id + 1)] += anc_min_time[anc_id] - current_time\n",
    "            assert new_anc_times[1] > np.max(sample_times)  # root ancestor\n",
    "\n",
    "        with self.__class__(  # Create new AncestorData instance to return\n",
    "            variant_data.sites_position[:][used_sites],\n",
    "            variant_data.sequence_length,\n",
    "            **kwargs,\n",
    "        ) as other:\n",
    "            mutated_sites = set()  # To check if mutations have occurred yet\n",
    "            ancestors_iter = self.ancestors()\n",
    "            anc = next(ancestors_iter, None)\n",
    "            for i in reverse_time_sorted_indexes:\n",
    "                proxy_time = proxy_times[i]\n",
    "                sample_id = sample_ids[i]\n",
    "                haplotype = haplotypes[i]\n",
    "                derived_sites = set(np.where(haplotype > 0)[0])\n",
    "                while anc is not None and new_anc_times[anc.id] > proxy_time:\n",
    "                    anc_time = new_anc_times[anc.id]\n",
    "                    other.add_ancestor(\n",
    "                        anc.start, anc.end, anc_time, anc.focal_sites, anc.haplotype)\n",
    "                    mutated_sites.update(anc.focal_sites)\n",
    "                    anc = next(ancestors_iter, None)\n",
    "                if not derived_sites.issubset(mutated_sites):\n",
    "                    assert not keep_ancestor_times\n",
    "                    logging.info(\n",
    "                        f\"Infinite sites assumption deliberately broken: {sample_id}\"\n",
    "                        \"contains an allele which requires a novel mutation.\"\n",
    "                    )\n",
    "                logger.debug(\n",
    "                    f\"Inserting proxy ancestor: sample {sample_id} at time {proxy_time}\"\n",
    "                )\n",
    "                other.add_ancestor(\n",
    "                    start=0,\n",
    "                    end=self.num_sites,\n",
    "                    time=proxy_time,\n",
    "                    focal_sites=[],\n",
    "                    haplotype=haplotype,\n",
    "                )\n",
    "            # Add any ancestors remaining in the current instance\n",
    "            while anc is not None:\n",
    "                anc_time = new_anc_times[anc.id]\n",
    "                other.add_ancestor(\n",
    "                    anc.start, anc.end, anc_time, anc.focal_sites, anc.haplotype,  \n",
    "                )\n",
    "                anc = next(ancestors_iter, None)\n",
    "\n",
    "            other.clear_provenances()\n",
    "            for timestamp, record in self.provenances():\n",
    "                other.add_provenance(timestamp, record)\n",
    "            other.record_provenance(command=\"insert_proxy_samples\", **kwargs)\n",
    "\n",
    "        assert other.num_ancestors == self.num_ancestors + len(sample_ids)\n",
    "        return other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5bb3d-1ea3-405f-b545-6d618b6327d8",
   "metadata": {},
   "source": [
    "### simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0232a72-4136-4e8b-8006-8bcd107f1041",
   "metadata": {},
   "source": [
    "simulations of a simple evolutionary dynamics in which genomes of a well-mixed population of fixed size {Ne} evolve under reproduction, (neutral) mutation at a fixed rate {mu}\u0016 per base per generation, and homologous recombination at a rate {rr}\u001a per base per gen- eration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e37680-1a7e-4eed-89d8-c87f4f3851c3",
   "metadata": {},
   "source": [
    "Base parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb0d69de-5cd2-4fa4-88ae-cc007764bd17",
   "metadata": {},
   "source": [
    "ne = 5000\n",
    "sample_size = 60 across time points\n",
    "seed = 5\n",
    "L = 3e6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe377b5-65c4-4d70-8eca-76b05aebcfe3",
   "metadata": {},
   "source": [
    "### Determining biologically plausible parameters\n",
    "In the literature, recombination is usually expressed in fractions p/m (recombination rate/mutation rate).  \n",
    "I will vary mutation rates across some range, and choose recombination rates such that i have a grid of p/m 0, 0.1, 0.01, 0.001, 0.3, 1, and 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959e88b-c6b9-4215-b121-920e5e572123",
   "metadata": {},
   "source": [
    "Sweeps parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2771fd2f-6a88-4bf7-95a2-cbc87d8930a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#later: tract lengths\n",
    "#gcrs = {}\n",
    "#gcrls = {} \n",
    "#times = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858960c1-6281-4dfd-8f00-3fc63dcb5611",
   "metadata": {},
   "source": [
    "## simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6a7df1-af3d-4540-abc7-d1b1f292acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_time_days = 1.0 \n",
    "gen_per_year = 365.0 \n",
    "\n",
    "def years_to_gen(years): \n",
    "    return years*gen_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b684d5-1f04-4e12-b841-f08ab60559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " def sim(ne, L, rr, mu, seed): \n",
    "\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples = [\n",
    "            msprime.SampleSet(20, time=0, ploidy = 1), \n",
    "            msprime.SampleSet(15, time=years_to_gen(5), ploidy = 1), \n",
    "            msprime.SampleSet(15, time=years_to_gen(10), ploidy = 1), \n",
    "            msprime.SampleSet(10, time=years_to_gen(25), ploidy = 1), \n",
    "            msprime.SampleSet(5, time=years_to_gen(50), ploidy = 1)\n",
    "        ],\n",
    "        sequence_length=L,\n",
    "        recombination_rate=rr,    \n",
    "        population_size=ne,\n",
    "        random_seed=seed,\n",
    "    )\n",
    "    \n",
    "    ts = msprime.sim_mutations(ts, rate=mu, random_seed = seed)\n",
    "\n",
    "    dated_ts = tsdate.date(ts, \n",
    "                       mutation_rate=mu, # same mutation rate used for simulation \n",
    "                       time_units=\"generations\", # dont want to switch this or nodes and samples will have different units\n",
    "                       match_segregating_sites = True,\n",
    "                       rescaling_intervals = 1\n",
    "                       )\n",
    "    \n",
    "    samples = list(dated_ts.samples()) ######## return \n",
    "\n",
    "    # get ancestral states\n",
    "    ancestral_states = []\n",
    "    \n",
    "    for site in ts.sites():\n",
    "        if site.ancestral_state is None:\n",
    "            ancestral_states.append(\"N\")\n",
    "            #print(\"N\")\n",
    "        else:\n",
    "            ancestral_states.append(str(site.ancestral_state))\n",
    "            #print(site.ancestral_state)\n",
    "    \n",
    "    ancestral_states = np.array(ancestral_states) ######### return\n",
    "\n",
    "    return dated_ts, samples, ancestral_states  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f545b93-480e-48cd-9936-64c0634c3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sweep(mu_vals, pm_grid): \n",
    "    # create empty list.\n",
    "    sims = [] #save each ts here \n",
    "    samples_list = [] \n",
    "    ancestral_states_list = []\n",
    "    metadata = [] \n",
    "    count = 0\n",
    "    counts = []\n",
    "    times = [] \n",
    "\n",
    "    for mu in mu_vals:\n",
    "        for pm in pm_grid:\n",
    "            count += 1\n",
    "            rr = mu * pm ####### change this so it's not 1.0000000001e-X \n",
    "            dated_ts, samples, ancestral_states = sim(ne, L, rr, mu)\n",
    "            \n",
    "            sims.append(dated_ts) \n",
    "            samples_list.append(samples) \n",
    "            ancestral_states_list.append(ancestral_states)\n",
    "            times.append(dated_ts.nodes_time[0:65])\n",
    "\n",
    "            metadata.append({\"index\": count-1, \"rate\": rr, \"mu\": mu})\n",
    "\n",
    "            counts.append({\"rr\": rr, \"mu\": mu, \"num_trees\": dated_ts.num_trees, \"diversity\": dated_ts.diversity()}) \n",
    "\n",
    "            print(f\"Finished inference {count}/{(len(mus)*len(pm_grid))}. RR: {rr}, MU: {mu}, num trees: {dated_ts.num_trees}\")\n",
    "\n",
    "    return pd.DataFrame(counts), pd.DataFrame(metadata), sims, samples_list, ancestral_states_list, times\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1302b1-adb4-4e9d-9078-1a7583c0b9cb",
   "metadata": {},
   "source": [
    "## vcf export/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e167d5eb-fd64-437a-af81-6f2d3a2571f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_sci(x):\n",
    "    s = f\"{x:.3e}\"\n",
    "    s = s.replace(\"+0\", \"+\").replace(\"-0\", \"-\") # reformatting numbers\n",
    "    return s\n",
    "\n",
    "def vcz_name(prefix, mu, rr, seed):\n",
    "    return f\"{prefix}_mu{fmt_sci(mu)}_rr{fmt_sci(rr)}_seed{seed}.vcf.gz.icf.vcz\"\n",
    "\n",
    "# export\n",
    "def check_zarr_store(p):\n",
    "    # version differences? \n",
    "    return (\n",
    "        os.path.isdir(p)\n",
    "        and (\n",
    "            os.path.exists(os.path.join(p, \".zgroup\"))\n",
    "            or os.path.exists(os.path.join(p, \"zarr.json\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def export_sim(prefix, ts, mu, rr, seed, workdir=\".\", force=False):\n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "    base = os.path.join(workdir, vcz_name(prefix, mu, rr, seed))             # ...vcf.gz.icf.vcz\n",
    "    vcf_path   = base.replace(\".vcf.gz.icf.vcz\", \".vcf\")\n",
    "    vcfgz_path = base.replace(\".icf.vcz\", \"\")                                  # ...vcf.gz\n",
    "    icf_path   = base.replace(\".vcz\", \"\")   # ...vcf.gz.icf\n",
    "\n",
    "    if os.path.exists(base):\n",
    "        return os.path.abspath(base)\n",
    "    \n",
    "    \n",
    "    # if .vcz exists, skip all\n",
    "    # if os.path.exists(base):\n",
    "    #     if check_zarr_store(base):\n",
    "    #         return os.path.abspath(base)\n",
    "\n",
    "    if os.path.exists(base):\n",
    "        if check_zarr_store(base):\n",
    "            return os.path.abspath(base)\n",
    "        if os.path.isdir(base):\n",
    "            shutil.rmtree(base, ignore_errors=True)\n",
    "        else:\n",
    "            os.remove(base)\n",
    "\n",
    "    # if .icf exists, encode .vcz\n",
    "    if os.path.exists(icf_path):\n",
    "        cmd = [\"vcf2zarr\", \"encode\", icf_path, base]\n",
    "        if force: cmd.append(\"--force\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "        return os.path.abspath(base)\n",
    "\n",
    "    # if .vcf.gz, explode -> .icf and encode -> .vcz\n",
    "    if os.path.exists(vcfgz_path):\n",
    "        cmd = [\"vcf2zarr\", \"explode\", vcfgz_path, icf_path]\n",
    "        if force: cmd.append(\"--force\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "        cmd = [\"vcf2zarr\", \"encode\", icf_path, base]\n",
    "        if force: cmd.append(\"--force\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "        return os.path.abspath(base)\n",
    "\n",
    "    # create .vcf, compress \n",
    "    if not os.path.exists(vcfgz_path):\n",
    "        with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=workdir) as tmp:\n",
    "            tmp_vcf = tmp.name\n",
    "            ts.write_vcf(tmp, position_transform=lambda x: np.fmax(1, x))\n",
    "        # compress\n",
    "        if shutil.which(\"bgzip\"):\n",
    "            subprocess.run([\"bgzip\", \"-f\", tmp_vcf], check=True)\n",
    "            # bgzip makes tmp_vcf + .gz\n",
    "            os.replace(tmp_vcf + \".gz\", vcfgz_path)\n",
    "        else:\n",
    "            with open(tmp_vcf, \"rb\") as fin, gzip.open(vcfgz_path, \"wb\") as fout:\n",
    "                fout.write(fin.read())\n",
    "            os.remove(tmp_vcf)\n",
    "\n",
    "    # explode vcf.gz to icf\n",
    "    if not os.path.exists(icf_path):\n",
    "        cmd = [\"vcf2zarr\", \"explode\", vcfgz_path, icf_path]\n",
    "        if force: cmd.append(\"--force\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    # encode icf to vzc\n",
    "    if not os.path.exists(base):\n",
    "        cmd = [\"vcf2zarr\", \"encode\", icf_path, base]\n",
    "        if force: cmd.append(\"--force\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    return os.path.abspath(base)\n",
    "\n",
    "\n",
    "# import vcz -> variant data object\n",
    "def import_sim(vcz_path, ancestral_states, individuals_time):\n",
    "    import tsinfer\n",
    "    vdata = tsinfer.VariantData(\n",
    "        vcz_path,\n",
    "        ancestral_state=np.asarray(ancestral_states),\n",
    "        individuals_time=np.asarray(individuals_time),\n",
    "    )\n",
    "    return vdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec39b71-0c83-480d-9c94-52843c62ee54",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17d5393-8e53-4346-8139-8439791f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsinfer.AncestorData.insert_proxy_samples = insert_proxy_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10798be9-bc41-4257-8e7e-957626020220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proxy(vdata, mu, rr_value=None, mm_value=None):\n",
    "    if rr_value is not None:\n",
    "        rr = rr_value \n",
    "        mm = mm_value\n",
    "    else: \n",
    "        rr = None\n",
    "        mm = None\n",
    "\n",
    "    anc = tsinfer.generate_ancestors(vdata)\n",
    "    anc_proxy = anc.insert_proxy_samples(vdata)\n",
    "    anc_proxy_ts = tsinfer.match_ancestors(vdata, anc_proxy, recombination_rate=rr, mismatch_ratio=mm)\n",
    "    ts_proxy = tsinfer.match_samples(vdata, anc_proxy_ts, force_sample_times=True, recombination_rate=rr)\n",
    "    simplified_proxy = tsdate.preprocess_ts(ts_proxy, erase_flanks=False)\n",
    "\n",
    "    dated_proxy = tsdate.date(\n",
    "        simplified_proxy,\n",
    "        mutation_rate=mu,\n",
    "        time_units=\"generations\",\n",
    "        match_segregating_sites=True,\n",
    "        rescaling_intervals=1,\n",
    "    )\n",
    "    return rr, mm, dated_proxy\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdbc2b-5139-48a2-9a0d-5c472e7831ba",
   "metadata": {},
   "source": [
    "### get tree width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94bc2797-f36b-4503-8dcf-bd5c3bdc8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(dated_ts): \n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for tree in dated_ts.trees():\n",
    "        left, right = tree.interval\n",
    "        data.append({\n",
    "            \"tree_index\": tree.index,\n",
    "            \"left\": left,\n",
    "            \"right\": right,\n",
    "        })\n",
    "    \n",
    "    intervals = pd.DataFrame(data)\n",
    "\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81398a69-36fa-4459-b411-4bcc540d007d",
   "metadata": {},
   "source": [
    "### get mrcas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b11579-d54f-4283-8e55-cf25d036f858",
   "metadata": {},
   "source": [
    "Returns a dataframe of pairwise MRCAs for every combination of samples, for each tree within the simulated tree sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98534e9-8518-40ea-bced-f84375766bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims_times(dated_ts, samples):\n",
    "    res = []\n",
    "\n",
    "    for a, b in combinations(samples, 2):\n",
    "        for i in range(0, dated_ts.get_num_trees()):\n",
    "            t = dated_ts.at_index(i).tmrca(a, b)\n",
    "            w = dated_ts.at_index(i).interval.right - dated_ts.at_index(i).interval.left\n",
    "            l = dated_ts.at_index(i).interval.left\n",
    "            r = dated_ts.at_index(i).interval.right\n",
    "            num_trees = dated_ts.num_trees\n",
    "            res.append({\"index\": i, \"sample_a\": a, \"sample_b\": b, \"mrca\": t, \"width\": w, \"left\": l, \"right\": r, \"num_trees_sim\": num_trees})\n",
    "            \n",
    "    mrcas = pd.DataFrame(res) \n",
    "    return mrcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0efb4c1-2685-4c14-8265-f00ca2c990af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_times(dated_ts, samples):\n",
    "    res = []\n",
    "\n",
    "    for a, b in combinations(samples, 2):\n",
    "        for i in range(1, dated_ts.get_num_trees()-1): #flanking trees are not informative and must be trimmed\n",
    "            t = dated_ts.at_index(i).tmrca(a, b)\n",
    "            w = dated_ts.at_index(i).interval.right - dated_ts.at_index(i).interval.left\n",
    "            l = dated_ts.at_index(i).interval.left\n",
    "            r = dated_ts.at_index(i).interval.right\n",
    "            num_trees = dated_ts.num_trees\n",
    "            res.append({\"index\": i, \"sample_a\": a, \"sample_b\": b, \"mrca\": t, \"width\": w, \"left\": l, \"right\": r, \"num_trees\": num_trees})\n",
    "            \n",
    "    mrcas = pd.DataFrame(res) \n",
    "    return mrcas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098aa406-da7b-41d4-b00b-35bb5821ac7e",
   "metadata": {},
   "source": [
    "### varying recombination rate and mismatch_ratio during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7809ee6-945f-420b-819e-6b37278c8769",
   "metadata": {},
   "source": [
    "Produces an array of recombination rates and mismatch ratios , then calls run_inference to infer tree sequences under varying parameters. Returns counts (for indexing), metadata (tree index, rr and mm values), and seqs(a list of tree sequences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c071a5-5e6a-4860-81de-56e5cec5ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_mm(vdata, mu): \n",
    "\n",
    "    # pm_grid = np.array([1e-3, 1e-2, 0.1, 0.3, 1.0, 10.0]) # bioligically plausible r/m sweep\n",
    "    pm_grid = np.array([1e-3, 1e-2, 0.1, 0.3, 1.0])\n",
    "    rates = list(mu * pm_grid)\n",
    "    \n",
    "    mms = [10**x for x in range(-3, 1, 1)]\n",
    "\n",
    "    n_r, n_m = len(rates), len(mms)\n",
    "    grid_num_trees = np.full((n_r, n_m), np.nan)\n",
    "    \n",
    "    seqs = [] #save each ts here \n",
    "    \n",
    "    count = 0\n",
    "    nones = 0\n",
    "\n",
    "    metadata = [] \n",
    "    counts = []\n",
    "\n",
    "    print(f\"Begin inference for simulated mu = {mu}.\") \n",
    "    for rr_idx, rr_value in enumerate(rates):\n",
    "        for mm_idx, mm_value in enumerate(mms): \n",
    "\n",
    "            if nones > 0: \n",
    "                break\n",
    "\n",
    "            if rr_value == None:\n",
    "                mm_value = 1\n",
    "                nones += 1 \n",
    "            \n",
    "            count+=1\n",
    "            \n",
    "            rr, mm, ip = run_proxy(vdata, rr_value, mm_value)\n",
    "            \n",
    "            grid_num_trees[rr_idx][mm_idx] = ip.num_trees\n",
    "            \n",
    "            seqs.append(ip) \n",
    "\n",
    "            metadata.append({\"index\": count-1, \"rate\": rr, \"mm\": mm})\n",
    "\n",
    "            counts.append({\"rate\": rr, \"mm\": mm, \"num_trees\": ip.num_trees}) \n",
    "\n",
    "            print(f\"Finished inference {count}/{(len(rates)*len(mms)+1)}. RR: {rr_value}, MM: {mm_value}, num trees: {ip.num_trees}\")\n",
    "\n",
    "    return pd.DataFrame(counts), pd.DataFrame(metadata), seqs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74562cda-f4b4-4307-8bc1-0273034395d6",
   "metadata": {},
   "source": [
    "# core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bf484-da8f-4b8e-80a3-cd48e464ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rr from pm grid\n",
    "def rr_for(mu, pm): \n",
    "    return pm * mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801f6f6-26b2-407c-913d-bbe9b4cf3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edges(L, bin_size):\n",
    "    return np.arange(0, int(L) + bin_size, bin_size, dtype=np.int64)\n",
    "\n",
    "def add_bins(df, positions):\n",
    "    out = []\n",
    "    \n",
    "    for i, pos in enumerate(positions):\n",
    "        mask = (df[\"left\"] <= pos) & (pos < df[\"right\"])   # half-open [left, right) like tskit intervals\n",
    "        if mask.any():\n",
    "            tmp = df.loc[mask].copy()\n",
    "            tmp[\"bin\"] = i\n",
    "            tmp[\"position\"] = int(pos)\n",
    "            out.append(tmp)\n",
    "            \n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "def align_mrcas(sim_df, inf_df, positions):\n",
    "    sim_b = add_bins(sim_df, positions)\n",
    "    inf_b = add_bins(inf_df, positions)\n",
    "    \n",
    "    merged = pd.merge(\n",
    "        inf_b, sim_b,\n",
    "        on=[\"sample_a\", \"sample_b\", \"bin\"],\n",
    "        suffixes=(\"_inf\", \"_sim\")\n",
    "    )\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def r2_log1p(x, y):\n",
    "    x = np.log1p(x)\n",
    "    y = np.log1p(y)\n",
    "    r = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "    return r**2\n",
    "\n",
    "def r2_by_bin(merged):\n",
    "    return (\n",
    "        merged\n",
    "        .groupby(\"bin\", sort=False)\n",
    "        .apply(lambda group: r2_log1p(group[\"mrca_inf\"], group[\"mrca_sim\"]))\n",
    "        .dropna()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41456ceb-6ffc-456f-b1d9-c4b056410f2f",
   "metadata": {},
   "source": [
    "evaluating tree sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bc5f4c3-c24f-4ad2-81ed-380f6abb0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random permuations of bin order \n",
    "\n",
    "def random_permutations(n_bins, n_reps, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    I = np.arange(n_bins, dtype=np.int32)\n",
    "    out = []\n",
    "    \n",
    "    while len(out) < n_reps:\n",
    "        p = rng.permutation(n_bins).astype(np.int32)\n",
    "        if np.any(p != I):        # allow some fixed points; switch to np.all for strict\n",
    "            out.append(p)\n",
    "            \n",
    "    return out\n",
    "\n",
    "def apply_bin_perm(merged_base, perm):\n",
    "    df = merged_base.copy()\n",
    "    key = df[[\"sample_a\",\"sample_b\",\"bin\",\"mrca_inf\"]].rename(\n",
    "        columns={\"bin\":\"src_bin\",\"mrca_inf\":\"mrca_inf_src\"}\n",
    "    )\n",
    "    \n",
    "    mapper = {i:int(j) for i,j in enumerate(perm)}\n",
    "    df = df.assign(src_bin=df[\"bin\"].map(mapper))\n",
    "    df = df.merge(key, on=[\"sample_a\",\"sample_b\",\"src_bin\"], how=\"left\", validate=\"many_to_one\")\n",
    "    df[\"mrca_inf\"] = df[\"mrca_inf_src\"].to_numpy()\n",
    "    \n",
    "    return df.drop(columns=[\"mrca_inf_src\",\"src_bin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6839b-3a0f-414f-a9ed-1afc045d221c",
   "metadata": {},
   "source": [
    "simulate + infer one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba888c4e-3256-4991-838d-30c6c6366ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dated ts, export VCZ\n",
    "def simulate_one(mu, rr, seed, prefix=\"sim\"):\n",
    "    # your sim() returns a dated tree sequence already\n",
    "    dated_ts, samples, ancestral_states = sim(ne, L, rr, mu, seed = seed)  # ground truth ts\n",
    "    vcz_path = export_sim(prefix, dated_ts, mu, rr, seed, workdir=\"vcf_stage\")\n",
    "    \n",
    "    return dated_ts, samples, ancestral_states, vcz_path\n",
    "\n",
    "# use simulated ts to make mrca table\n",
    "def mrca_table_sim(ts_sim):\n",
    "    # your MRCA extraction; rename 'mrca' to 'mrca_sim'\n",
    "    sim_df = get_sims_times(ts_sim, list(ts_sim.samples()))\n",
    "    \n",
    "    return sim_df.rename(columns={\"mrca\": \"mrca_sim\"})\n",
    "\n",
    "# re-import genotypes, run inference, compute inferred MRCA\n",
    "def infer_one(vcz_path, ancestral_states, individuals_time, mu, rr, seed, mm=1):\n",
    "    vdata = import_sim(vcz_path, ancestral_states, individuals_time)\n",
    "    rr_used, mm_used, dated_proxy = run_proxy(vdata, mu, rr_value=rr, mm_value=mm)\n",
    "    inf_df = get_res_times(dated_proxy, list(dated_proxy.samples()))\n",
    "    \n",
    "    return inf_df.rename(columns={\"mrca\": \"mrca_inf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce526e-ac20-41f9-b65d-f8f1591868dc",
   "metadata": {},
   "source": [
    "for parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3c979-69a7-4b30-80c5-b812420aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cell(mu, pm, rep, seed):\n",
    "    rr = pm * mu\n",
    "\n",
    "    # simulate + export VCF/VCZ\n",
    "    ts, samples, ancestral_states, vcz_path = simulate_one(mu, rr, seed=seed)\n",
    "\n",
    "    # build truth table from the simulated ts\n",
    "    edges = make_edges(int(ts.sequence_length), BIN_SIZE)\n",
    "    sim_df = mrca_table_sim(ts)\n",
    "\n",
    "    # per-individual times for import\n",
    "    sample_nodes = np.array(list(ts.samples()), dtype=int)\n",
    "    individuals_time = ts.tables.nodes.time[sample_nodes]\n",
    "\n",
    "    # inference w/ re-imported genomic data\n",
    "    inf_df = infer_one(vcz_path, ancestral_states, individuals_time, mu=mu, rr=rr, seed=seed, mm=1)\n",
    "\n",
    "    ##sim_df and inf_df have num_trees .... extract\n",
    "    n_trees_sim = int(sim_df[\"num_trees_sim\"].iloc[0])\n",
    "    n_trees_inf = int(inf_df[\"num_trees\"].iloc[0])\n",
    "\n",
    "\n",
    "    # align + R2\n",
    "    merged = align_mrcas(sim_df, inf_df, edges)\n",
    "    r2_inf = r2_by_bin(merged)\n",
    "    r2_vals = np.asarray(r2_inf.to_numpy()).ravel()\n",
    "    bins = np.asarray(r2_inf.index.to_numpy(), dtype=np.int32).ravel()\n",
    "    n = len(r2_vals)\n",
    "\n",
    "    #print(r2_vals)\n",
    "    \n",
    "    inf_rows = pd.DataFrame({\n",
    "        \"mu\":   np.full(n, mu, dtype=float),\n",
    "        \"pm\":   np.full(n, pm, dtype=float),\n",
    "        \"rr\":   np.full(n, rr, dtype=float),\n",
    "        \"rep\":  np.full(n, rep, dtype=int),\n",
    "        \"bin\":  bins,\n",
    "        \"r2\":   r2_vals,\n",
    "        \"kind\": np.full(n, \"inferred\", dtype=object),\n",
    "        \"n_trees_sim\": np.full(n, n_trees_sim, dtype=int),\n",
    "        \"n_trees_inf\": np.full(n, n_trees_inf, dtype=int) \n",
    "    })\n",
    "\n",
    "\n",
    "    # null: reuse merged; permute bins (no re-merge)\n",
    "    null_rows = []\n",
    "    if N_NULL > 0:\n",
    "        perms = random_permutations(n_bins=len(edges)-1, n_reps=N_NULL, seed=seed)\n",
    "        for p in perms:\n",
    "            m_perm = apply_bin_perm(merged, p)\n",
    "            r2_null = r2_by_bin(m_perm)\n",
    "            if len(r2_null):\n",
    "                null_rows.append(pd.DataFrame({\n",
    "                    \"mu\": mu, \n",
    "                    \"pm\": pm, \n",
    "                    \"rr\": rr, \n",
    "                    \"rep\": rep,\n",
    "                    \"bin\": r2_null.index.astype(np.int32),\n",
    "                    \"r2\": r2_null.values,\n",
    "                    \"kind\": \"null\"\n",
    "                }))\n",
    "    null_df = pd.concat(null_rows, ignore_index=True) if null_rows else pd.DataFrame(\n",
    "        columns=[\"mu\",\"pm\",\"rr\",\"rep\",\"bin\",\"r2\",\"kind\"]\n",
    "    )\n",
    "    return inf_rows, null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e21d8eb-18ef-4ac1-a1ac-149a18247c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "MUS     = [1e-10, 1e-9, 1e-8, 1e-7]   # 1e-6]            # per-site, per-gen\n",
    "PM_GRID = [1e-3, 1e-2, 0.1, 0.3, 1.0, 3.0]      #1.0] #, 10.0]         # p/m ratios\n",
    "BASE_SEED  = 50\n",
    "BIN_SIZE   = 100_000\n",
    "N_REPS     = 5 # per (mu, pm)\n",
    "N_NULL     = 10 # null perms per replicate\n",
    "#seed = 50\n",
    "MAX_WORKERS = max(1, (os.cpu_count() or 2) - 1)\n",
    "\n",
    "ne = 5000\n",
    "L  = int(3e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47e2baf0-2f1e-4323-92c7-b07ffb32580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sweep_parallel(mus, pm_grid, base_seed=BASE_SEED, max_workers=MAX_WORKERS):\n",
    "    futures = []\n",
    "    total = len(mus) * len(pm_grid) * N_REPS\n",
    "    desc = f\"Running {total} parameter combos\"\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for i, mu in enumerate(mus):\n",
    "            for j, pm in enumerate(pm_grid):\n",
    "                for rep in range(N_REPS):\n",
    "                    seed = base_seed + i*10000 + j*100 + rep*3\n",
    "                    futures.append(ex.submit(run_cell, mu, pm, rep, seed))\n",
    "\n",
    "        inf_parts, null_parts = [], []\n",
    "        with tqdm(total=total, desc=desc, ncols=100) as pbar:\n",
    "            for fut in as_completed(futures):\n",
    "                try:\n",
    "                    a, b = fut.result()\n",
    "                    inf_parts.append(a); null_parts.append(b)\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"worker failed: {e!r}\")\n",
    "                finally:\n",
    "                        pbar.update(1)\n",
    "\n",
    "    inf_all  = pd.concat(inf_parts,  ignore_index=True) if inf_parts  else pd.DataFrame()\n",
    "    null_all = pd.concat(null_parts, ignore_index=True) if null_parts else pd.DataFrame()\n",
    "    return inf_all, null_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364517aa-1662-4cb5-a910-9b72562423b5",
   "metadata": {},
   "source": [
    "## run all in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e1ed3b-8f66-489a-a30b-7b13a0e65b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af745365a474247a2d2ef31b8a7360c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 2 parameter combos:   0%|                                             | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Scan: 100%|██████████| 1.00/1.00 [00:01<00:00, 1.44s/files]\n",
      "    Scan: 100%|██████████| 1.00/1.00 [00:01<00:00, 1.45s/files]\n",
      "2025-12-09 08:56:53 gizmoj32 bio2zarr.vcf[24322] WARNING Total records unknown, cannot show progress; reindex VCFs with bcftools index to fix\n",
      " Explode: 0.00vars [00:00, ?vars/s]2025-12-09 08:56:53 gizmoj32 bio2zarr.vcf[24321] WARNING Total records unknown, cannot show progress; reindex VCFs with bcftools index to fix\n",
      " Explode: 65.0vars [00:00, 218vars/s]]\n",
      " Explode: 607vars [00:00, 1.58kvars/s]\n",
      "  Encode: 100%|██████████| 15.0k/15.0k [00:00<00:00, 22.1kB/s]\n",
      "  Encode: 100%|██████████| 140k/140k [00:00<00:00, 185kB/s] \n",
      "Finalise: 100%|██████████| 11.0/11.0 [00:00<00:00, 67.7array/s]\n",
      "Finalise: 100%|██████████| 11.0/11.0 [00:00<00:00, 67.6array/s]\n"
     ]
    }
   ],
   "source": [
    "# run \n",
    "inf_all, null_all = sim_sweep_parallel(MUS, PM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "893307ba-7a79-4fbf-9c86-f6d76cb4e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_all.to_csv('inf_all_reps.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2570f841-673c-45a3-aac2-80256f6bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_all.to_csv('null_all_reps.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsinfer_kernel)",
   "language": "python",
   "name": "tsinfer_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
